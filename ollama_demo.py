"""
ä½¿ç”¨Ollamaå…è´¹æœ¬åœ°AIè¿›è¡Œæ•°æ®åˆ†æ

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨å®Œå…¨å…è´¹çš„Ollamaæœ¬åœ°æ¨¡å‹è¿›è¡Œæ•°æ®åˆ†æ
"""
import json
import requests

def test_ollama_chat():
    """æµ‹è¯•OllamaèŠå¤©åŠŸèƒ½"""
    print("ğŸ¤– æµ‹è¯•Ollama AIå¯¹è¯...")
    
    # æµ‹è¯•åŸºæœ¬å¯¹è¯
    url = "http://localhost:11434/api/generate"
    data = {
        "model": "codellama:7b",
        "prompt": "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚ç”¨ä¸­æ–‡å›ç­”ã€‚",
        "stream": False
    }
    
    try:
        response = requests.post(url, json=data, timeout=30)
        if response.status_code == 200:
            result = response.json()
            print("âœ… AIå›å¤:")
            print(result.get('response', 'æ— å›å¤'))
            return True
        else:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ å¯¹è¯æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_data_analysis():
    """æµ‹è¯•æ•°æ®åˆ†æåŠŸèƒ½"""
    print("\nğŸ“Š æµ‹è¯•æ•°æ®åˆ†æ...")
    
    # æ¨¡æ‹Ÿæ•°æ®åˆ†æé—®é¢˜
    url = "http://localhost:11434/api/generate"
    data = {
        "model": "codellama:7b",
        "prompt": """
æˆ‘æœ‰ä»¥ä¸‹é”€å”®æ•°æ®ï¼š
é”€å”®å‘˜: å¼ ä¸‰, é”€å”®é¢: 10000
é”€å”®å‘˜: æå››, é”€å”®é¢: 15000  
é”€å”®å‘˜: ç‹äº”, é”€å”®é¢: 8000

è¯·åˆ†æå“ªä¸ªé”€å”®å‘˜è¡¨ç°æœ€å¥½ï¼Œå¹¶ç”¨ä¸­æ–‡ç®€å•è§£é‡Šã€‚
""",
        "stream": False
    }
    
    try:
        response = requests.post(url, json=data, timeout=30)
        if response.status_code == 200:
            result = response.json()
            print("âœ… æ•°æ®åˆ†æç»“æœ:")
            print(result.get('response', 'æ— åˆ†æç»“æœ'))
            return True
        else:
            print(f"âŒ åˆ†æå¤±è´¥: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ æ•°æ®åˆ†æå¤±è´¥: {e}")
        return False

def show_usage_guide():
    """æ˜¾ç¤ºä½¿ç”¨æŒ‡å—"""
    print("\n" + "="*50)
    print("ğŸ¯ Ollamaä½¿ç”¨æŒ‡å—")
    print("="*50)
    
    print("\nâœ… å½“å‰çŠ¶æ€:")
    print("  - OllamaæœåŠ¡: è¿è¡Œä¸­")
    print("  - æ¨¡å‹: CodeLlama 7B")
    print("  - æˆæœ¬: å®Œå…¨å…è´¹")
    print("  - éšç§: æ•°æ®ä¸ä¼šä¸Šä¼ åˆ°ç½‘ç»œ")
    
    print("\nğŸ”§ åŸºæœ¬ä½¿ç”¨:")
    print("  - ç›´æ¥å¯¹è¯: ollama run codellama:7b")
    print("  - åœæ­¢æœåŠ¡: Ctrl+C ç»ˆæ­¢ ollama serve")
    print("  - æŸ¥çœ‹æ¨¡å‹: ollama list")
    
    print("\nğŸ“ Pythonä»£ç é›†æˆ:")
    print("""
# åŸºæœ¬å¯¹è¯ç¤ºä¾‹
import requests

def chat_with_ollama(question):
    url = "http://localhost:11434/api/generate"
    data = {
        "model": "codellama:7b",
        "prompt": question,
        "stream": False
    }
    response = requests.post(url, json=data)
    return response.json().get('response', '')

# ä½¿ç”¨ç¤ºä¾‹
answer = chat_with_ollama("åˆ†æè¿™ç»„æ•°æ®: [1,2,3,4,5]")
print(answer)
""")
    
    print("\nğŸš€ ä¸pandas-aié›†æˆ:")
    print("1. ä¿®å¤pandasç‰ˆæœ¬é—®é¢˜")
    print("2. é…ç½®pandas-aiä½¿ç”¨Ollama")
    print("3. å¼€å§‹æ•°æ®åˆ†æ")

def create_ollama_config_example():
    """åˆ›å»ºOllamaé…ç½®ç¤ºä¾‹"""
    print("\nğŸ“„ åˆ›å»ºé…ç½®æ–‡ä»¶ç¤ºä¾‹...")
    
    config_content = '''"""
Ollama + pandas-ai é…ç½®ç¤ºä¾‹
"""
import requests
from typing import Any, Dict

class OllamaLLM:
    """ç®€å•çš„Ollama LLMåŒ…è£…å™¨"""
    
    def __init__(self, model: str = "codellama:7b", base_url: str = "http://localhost:11434"):
        self.model = model
        self.base_url = base_url
    
    def generate(self, prompt: str) -> str:
        """ç”Ÿæˆå›å¤"""
        url = f"{self.base_url}/api/generate"
        data = {
            "model": self.model,
            "prompt": prompt,
            "stream": False
        }
        
        try:
            response = requests.post(url, json=data, timeout=30)
            if response.status_code == 200:
                return response.json().get('response', '')
            else:
                return f"é”™è¯¯: {response.status_code}"
        except Exception as e:
            return f"è¯·æ±‚å¤±è´¥: {e}"

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    llm = OllamaLLM()
    
    # æµ‹è¯•å¯¹è¯
    questions = [
        "ä½ å¥½ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”",
        "ä»€ä¹ˆæ˜¯æ•°æ®åˆ†æï¼Ÿ",
        "å¦‚ä½•è®¡ç®—å¹³å‡å€¼ï¼Ÿ"
    ]
    
    for q in questions:
        print(f"é—®é¢˜: {q}")
        answer = llm.generate(q)
        print(f"å›ç­”: {answer}\\n")
'''
    
    with open("/Users/longwaystov2025/dev_workspaces/pandas-ai/ollama_config_example.py", "w", encoding="utf-8") as f:
        f.write(config_content)
    
    print("âœ… é…ç½®ç¤ºä¾‹å·²ä¿å­˜åˆ°: ollama_config_example.py")

def main():
    print("ğŸ‰ æ­å–œï¼ä½ çš„å…è´¹AIå·²ç»å‡†å¤‡å°±ç»ª")
    print("="*50)
    
    # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    if test_ollama_chat():
        print("\nâœ… åŸºæœ¬å¯¹è¯åŠŸèƒ½æ­£å¸¸")
    
    if test_data_analysis():
        print("\nâœ… æ•°æ®åˆ†æåŠŸèƒ½æ­£å¸¸")
    
    # æ˜¾ç¤ºæŒ‡å—
    show_usage_guide()
    
    # åˆ›å»ºé…ç½®ç¤ºä¾‹
    create_ollama_config_example()
    
    print("\nğŸ¯ æ€»ç»“:")
    print("âœ… ä½ ç°åœ¨æ‹¥æœ‰äº†ä¸€ä¸ªå®Œå…¨å…è´¹çš„AIåŠ©æ‰‹")
    print("âœ… æ— éœ€APIå¯†é’¥ï¼Œæ— ä½¿ç”¨æ¬¡æ•°é™åˆ¶")
    print("âœ… æ•°æ®éšç§å®Œå…¨ä¿æŠ¤")
    print("âœ… å¯ä»¥è¿›è¡Œä»£ç ç”Ÿæˆã€æ•°æ®åˆ†æç­‰ä»»åŠ¡")
    
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
    print("1. å°è¯•è¿è¡Œ: ollama run codellama:7b")
    print("2. æµ‹è¯•ä¸åŒçš„æé—®æ–¹å¼")
    print("3. é›†æˆåˆ°ä½ çš„Pythoné¡¹ç›®ä¸­")

if __name__ == "__main__":
    main()